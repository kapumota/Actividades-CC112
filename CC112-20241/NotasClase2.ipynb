{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bb7713",
   "metadata": {},
   "source": [
    "## Análisis de algoritmos \n",
    "\n",
    "El análisis de algoritmos es un proceso fundamental en informática y matemáticas que se enfoca en evaluar la eficiencia de los algoritmos. Este análisis busca determinar los recursos que el algoritmo necesita para completar su tarea, siendo los más comunes el tiempo de ejecución (cuánto tarda en terminar) y el espacio de memoria (cuánta memoria usa). A través de este análisis, los desarrolladores e investigadores pueden comparar algoritmos, prever su comportamiento en diferentes condiciones y elegir la mejor opción para una tarea específica. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fc091",
   "metadata": {},
   "source": [
    "### Análisis de casos\n",
    "\n",
    "* Peor caso: La máxima cantidad de tiempo y espacio que un algoritmo puede requerir. Es útil para garantizar que el algoritmo se ejecutará dentro de recursos aceptables en el peor de los escenarios.\n",
    "\n",
    "* Caso promedio : Un promedio del tiempo y espacio requeridos, basado en una distribución de todas las posibles entradas. Es más representativo del rendimiento esperado en la práctica.\n",
    "\n",
    "* Mejoe caso: El mínimo tiempo y espacio requeridos. Aunque puede ser informativo, raramente es útil para la planificación de recursos porque se basa en el mejor escenario posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc8905",
   "metadata": {},
   "source": [
    "### Complejidad temporal\n",
    "\n",
    "La complejidad temporal se refiere a cómo cambia el tiempo de ejecución de un algoritmo a medida que aumenta el tamaño de entrada. Generalmente se expresa en términos de notación Big $O$ ($O$ grande), que ofrece un límite superior sobre el crecimiento del tiempo de ejecución. Por ejemplo, un algoritmo con una complejidad temporal de $O(n)$ tardará un tiempo proporcionalmente más largo a medida que el tamaño de la entrada `n` aumente.\n",
    "Se denota una complejidad temporal donde los tres puntos representan alguna función. Normalmente, la variable `n` denota el tamaño de entrada. \n",
    "\n",
    "Por ejemplo, si la entrada es un arreglo de números, `n` será el tamaño del arreglo y si la entrada es una cadena, `n` será la longitud de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8face58b",
   "metadata": {},
   "source": [
    "#### Complejidad constante\n",
    "\n",
    "Cuando hablamos de una complejidad de tiempo o espacio de $O(1)$, nos referimos a que es \"constante\". Esto significa que el tiempo o el espacio requerido por el algoritmo para ejecutarse no cambia con el tamaño de la entrada. Independientemente de cuán grande o pequeña sea la entrada, el algoritmo consumirá el mismo tiempo o espacio.\n",
    "\n",
    "**Características**\n",
    "\n",
    "Independencia del tamaño de entrada: La ejecución del algoritmo no se ve afectada por el tamaño de los datos de entrada. Por ejemplo, acceder a cualquier elemento de un arreglo por su índice es una operación de tiempo $O(1)$ porque se realiza en un paso, sin importar el tamaño del arreglo.\n",
    "\n",
    "Eficiencia: Los algoritmos con complejidad $O(1)$ son considerados muy eficientes, ya que el tiempo o espacio que requieren no aumenta con el tamaño de la entrada.\n",
    "\n",
    "**Ejemplos Comunes**\n",
    "\n",
    "* Acceder a un elemento de un arreglo por su índice.\n",
    "* Insertar o eliminar un elemento en una posición específica de una lista enlazada, asumiendo que tienes un puntero directo al nodo en cuestión.\n",
    "\n",
    "La complejidad $O(1)$ es un objetivo deseable cuando se diseñan estructuras de datos o algoritmos, ya que garantiza una previsibilidad en el rendimiento que no está vinculada al volumen de datos procesados. Sin embargo, no todos los problemas pueden resolverse de manera que su solución sea $O(1)$, especialmente aquellos que naturalmente requieren procesar cada elemento de la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4496c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a++;\n",
    "b++;\n",
    "c = a+b;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6f33d",
   "metadata": {},
   "source": [
    "#### Complejidad lineal\n",
    "Cuando hablamos de una complejidad de $O(n)$, nos referimos a \"lineal\". Esto significa que el tiempo o espacio que un algoritmo requiere crece de manera proporcional al tamaño de la entrada, $n$. A medida que el tamaño de la entrada aumenta, el tiempo de ejecución o el espacio utilizado aumenta en una cantidad directamente proporcional.\n",
    "\n",
    "La complejidad temporal de un bucle estima la cantidad de veces que se ejecuta el código dentro del bucle. Por ejemplo, la complejidad temporal del siguiente código es $O(n)$, porque el código dentro del bucle se ejecuta `n` veces. Suponemos que `\"...\"` denota un código cuya complejidad temporal es $O(1)$.\n",
    "\n",
    "**Características**\n",
    "- La cantidad de trabajo (tiempo de ejecución) o recursos (espacio) aumenta linealmente con el incremento en el tamaño de la entrada. Si duplicas el tamaño de la entrada, el tiempo o espacio necesario se duplicará aproximadamente.\n",
    "- Los algoritmos de complejidad $O(n)$ son generalmente eficientes para tamaños de entrada pequeños a moderados, pero su rendimiento puede degradarse con entradas muy grandes.\n",
    "\n",
    "**Ejemplos Comunes**\n",
    "\n",
    "La búsqueda lineal en un arreglo, donde se verifica cada elemento hasta encontrar el objetivo.\n",
    "Un bucle simple que recorre cada elemento de un arreglo o lista para realizar una operación, como sumar todos los valores.\n",
    "\n",
    "Aunque una complejidad $O(n)$ es menos ideal que $O(1)$, es común y aceptable para muchas aplicaciones, especialmente aquellas donde el tamaño de la entrada no es masivo o donde las operaciones dentro del bucle son relativamente simples. Los diseñadores de algoritmos y estructuras de datos siempre buscan minimizar la complejidad, pero para muchos problemas, $O(n)$ es la complejidad más baja posible.\n",
    "\n",
    "Por ejemplo, cualquier problema que requiera examinar cada elemento de una lista al menos una vez no puede tener una complejidad mejor que $O(n)$ porque cada elemento debe procesarse individualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203701db",
   "metadata": {},
   "source": [
    "#### Complejidad cuadrática\n",
    "\n",
    "La complejidad cuadrática, denotada como $O(n^2)$, es un término utilizado en el análisis de algoritmos para describir un algoritmo cuyo tiempo de ejecución o espacio necesario aumenta proporcionalmente al cuadrado del tamaño de la entrada $n$. Esto significa que si el tamaño de la entrada se duplica, el tiempo de ejecución o el espacio utilizado se cuadruplica. Los algoritmos con complejidad cuadrática son típicamente menos eficientes que aquellos con complejidades más bajas, como $O(n)$ o $O(\\log n)$, especialmente a medida que el tamaño de la entrada crece.\n",
    "\n",
    "**Características**\n",
    "\n",
    "- La cantidad de trabajo o recursos necesarios crece muy rápidamente con el aumento del tamaño de entrada. Esto hace que los algoritmos con complejidad $O(n^2)$ sean prácticos solo para tamaños de entrada relativamente pequeños.\n",
    "\n",
    "**Ejemplos**\n",
    "\n",
    "Algoritmos de ordenamiento como el ordenamiento de burbuja (bubble sort), ordenamiento por inserción (insertion sort), y ordenamiento por selección (selection sort) en sus implementaciones básicas.\n",
    "Algoritmos que implican una iteración doble sobre la entrada, como ciertos algoritmos de búsqueda y procesamiento de matrices.\n",
    "\n",
    "En el análisis de algoritmos, la presencia de bucles anidados (un bucle dentro de otro bucle) suele ser un indicador de complejidad $O(n^2)$, donde el bucle exterior e interior dependen del tamaño de la entrada.\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7da630",
   "metadata": {},
   "source": [
    "Entonces, la complejidad temporal del siguiente código es $O(n^2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    for (int j = 1; j <= n; j++) {\n",
    "        ...\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec34166",
   "metadata": {},
   "source": [
    "En general, si hay `k` bucles anidados y cada bucle pasa por `n` valores, la complejidad temporal es $O(n^k)$. Esto significa que el tiempo de ejecución aumenta exponencialmente con el número de niveles de bucles anidados, lo que puede llevar rápidamente a una eficiencia muy baja para valores grandes de $n$ y $k$.\n",
    "\n",
    "La identificación de la complejidad temporal como $O(n^k)$ en algoritmos es crucial para entender su escalabilidad y eficiencia. Algoritmos con esta forma de complejidad se vuelven rápidamente impracticables a medida que el tamaño de la entrada $n$ crece, especialmente si el número de bucles anidados $k$ es grande. Por lo tanto, los diseñadores de algoritmos buscan optimizar o reducir el valor de  $k$, simplificar la lógica dentro de los bucles, o encontrar enfoques algorítmicos completamente diferentes para manejar grandes conjuntos de datos de manera eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9d4fe",
   "metadata": {},
   "source": [
    "### Ejemplos\n",
    "\n",
    "En los siguientes ejemplos, el código dentro del bucle se ejecuta `3n`,`n + 5`  veces, pero la complejidad temporal de cada código es $O(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce912ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= 3*n; i++) {\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n+5; i++) {\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65060219",
   "metadata": {},
   "source": [
    "Como otro ejemplo, la complejidad temporal del siguiente código es $O(n^²)$ , porque el código dentro del bucle se ejecuta $1 +2 + \\dots +n = \\frac{1}{2}(n^2 + n)$ varias veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    for (int j = 1; j <= i; j++) {\n",
    "        ...\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622843f8",
   "metadata": {},
   "source": [
    "Si un algoritmo consta de fases consecutivas, la complejidad temporal total es la mayor complejidad temporal de una sola fase. La razón de esto es que la fase más lenta es el cuello de botella del algoritmo. Por ejemplo, el siguiente código consta de tres fases con complejidades temporales $O(n)$, $O(n^2)$ y $O(n)$. \n",
    "\n",
    "Por tanto, la complejidad del tiempo total es  $O(n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    ...\n",
    "}\n",
    "for (int i = 1; i <= n; i++) {\n",
    "    for (int j = 1; j <= n; j++) {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "for (int i = 1; i <= n; i++) {\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f8345",
   "metadata": {},
   "source": [
    "A veces, la complejidad del tiempo depende de varios factores y la fórmula de la complejidad del tiempo contiene varias variables. Por ejemplo, la complejidad temporal del siguiente código es $O(nm)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    for (int j = 1; j <= m; j++) {\n",
    "        ...\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a80039",
   "metadata": {},
   "source": [
    "**La complejidad temporal de una función recursiva depende de la cantidad de veces que se llama a la función y de la complejidad temporal de una sola llamada**. La complejidad del tiempo total es el producto de estos valores. Por ejemplo, considera la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2507f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "void f(int n) {\n",
    "    if (n == 1) return;\n",
    "    f(n-1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea87d9d",
   "metadata": {},
   "source": [
    "La llamada provoca `n` llamadas a funciones y la complejidad temporal de cada llamada es $O(1)$, por lo que la complejidad temporal total es $O(n)$.\n",
    "\n",
    "Como otro ejemplo, considera la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "void g(int n) {\n",
    "    if (n == 1) return;\n",
    "    g(n-1);\n",
    "    g(n-1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd692b",
   "metadata": {},
   "source": [
    "¿Qué sucede cuando se llama a la función con un parámetro `n`? \n",
    "\n",
    "Primero, hay dos llamadas con el parámetro `n-1`, luego cuatro llamadas con parámetro `n-2`, luego ocho llamadas con parámetro `n-3` y así sucesivamente. \n",
    "\n",
    "En general, habrá $2^k$ llamadas con el parámetro `n-k`,  donde $k = 0,1, \\dots,n -1$. \n",
    "\n",
    "Por tanto, la complejidad del tiempo es:\n",
    "\n",
    "$$1 + 2 + 4 + \\cdots + 2^{n-1} = 2^n -1 = O(2^n)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11d66c",
   "metadata": {},
   "source": [
    "#### Complejidades logarítmicas\n",
    "Las complejidades logarítmicas, denotadas comúnmente como $O(\\log n)$, son una clase importante en el análisis de algoritmos. Indican que el tiempo de ejecución de un algoritmo (o el espacio, aunque es menos común) aumenta logarítmicamente a medida que aumenta el tamaño de la entrada $n$. Esto significa que cada vez que el tamaño de la entrada se duplica, el tiempo de ejecución solo aumenta en una cantidad constante. Los algoritmos con complejidad logarítmica son altamente eficientes, especialmente para grandes volúmenes de datos.\n",
    "\n",
    "**Características** \n",
    "- Algoritmos con esta complejidad son muy eficientes ya que, incluso para entradas muy grandes, el incremento en el número de operaciones requeridas es relativamente pequeño.\n",
    "- La base del logaritmo en la notación $O(\\log n)$ generalmente no se especifica porque las complejidades se consideran hasta un factor constante. Sin embargo, en informática, el logaritmo suele ser en base 2 (debido a la naturaleza binaria de la computación), aunque matemáticamente, el cambio de base del logaritmo solo altera el resultado por un factor constante.\n",
    "\n",
    "**Ejemplos**\n",
    "\n",
    "La búsqueda binaria en un arreglo ordenado es un ejemplo clásico de un algoritmo $O(\\log n)$, donde se divide a la mitad el espacio de búsqueda con cada paso.\n",
    "Las operaciones en estructuras de datos balanceadas, como los árboles binarios de búsqueda balanceados (por ejemplo, AVL, árboles rojo-negro), donde las operaciones como búsqueda, inserción y eliminación pueden realizarse en tiempo logarítmico.\n",
    "\n",
    "Los algoritmos de complejidad logarítmica son particularmente valorados en el diseño de software y sistemas de procesamiento de datos debido a su eficiencia con grandes conjuntos de datos. Identificar o desarrollar algoritmos que operen en $O(\\log n)$ puede significar una diferencia sustancial en rendimiento, haciendo viables las operaciones en tiempo real que de otra manera serían demasiado lentas.\n",
    "\n",
    "**Búsqueda Binaria**:\n",
    "La búsqueda binaria ilustra bien el concepto de complejidad logarítmica. Dado un arreglo ordenado de $n$ elementos, la búsqueda binaria comienza en el medio del arreglo y compara el elemento de búsqueda con el valor medio; dependiendo de si el elemento buscado es mayor o menor que el valor medio, se continúa la búsqueda de manera recursiva en la mitad superior o inferior del arreglo. Este proceso se repite, dividiendo el espacio de búsqueda por la mitad en cada paso, hasta que se encuentra el elemento o hasta que el espacio de búsqueda se reduce a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba36055",
   "metadata": {},
   "source": [
    "```\n",
    "Función BúsquedaBinaria(Arreglo, elemento):\n",
    "    inicio = 0\n",
    "    fin = longitud(Arreglo) - 1\n",
    "\n",
    "    Mientras inicio <= fin:\n",
    "        medio = (inicio + fin) / 2\n",
    "        Si Arreglo[medio] == elemento:\n",
    "            Devolver medio\n",
    "        Si elemento < Arreglo[medio]:\n",
    "            fin = medio - 1\n",
    "        Sino:\n",
    "            inicio = medio + 1\n",
    "\n",
    "    Devolver -1  # El elemento no se encontró\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48874fab",
   "metadata": {},
   "source": [
    "La complejidad $O(n \\log n)$  es crucial en el análisis de algoritmos, especialmente en el contexto de algoritmos de ordenamiento y algunas operaciones avanzadas en estructuras de datos. Esta clase de complejidad combina aspectos lineales y logarítmicos, indicando que el tiempo de ejecución del algoritmo aumenta linealmente con el tamaño de la entrada $n$, pero cada operación lineal se multiplica por un factor logarítmico debido a la naturaleza de las operaciones internas.\n",
    "\n",
    "**Características**\n",
    "\n",
    "- Aunque $O(n \\log n)$ es más costoso que $O(n)$ o $O(\\log n)$, sigue siendo muy eficiente para muchos tipos de problemas, especialmente cuando comparado con complejidades cuadráticas $O(n^2)$ o peores. Esto hace que los algoritmos $O(n \\log n)$ sean prácticos y ampliamente utilizados para grandes volúmenes de datos.\n",
    "\n",
    "- Varios algoritmos de ordenamiento eficientes operan en tiempo $O(n \\log n)$ en el caso promedio o incluso en el peor de los casos. Estos incluyen mergesort, heapsort, y quicksort (aunque el peor caso de quicksort es $O(n^2)$, su caso promedio y su comportamiento práctico son generalmente $O(n \\log n)$.\n",
    "\n",
    "- Además del ordenamiento, la complejidad $O(n \\log n)$ aparece en otras operaciones complejas, como ciertas construcciones de árboles de búsqueda binaria balanceados, algoritmos de grafos, y en la computación de la Transformada Rápida de Fourier (FFT), que es fundamental para el procesamiento digital de señales.\n",
    "\n",
    "**Importancia**\n",
    "\n",
    "- Los algoritmos con complejidad $O(n \\log n)$ ofrecen un equilibrio entre complejidad puramente lineal y las más costosas complejidades polinomiales o exponenciales, permitiendo el manejo eficiente de grandes conjuntos de datos.\n",
    "\n",
    "- En el campo del ordenamiento y otras categorías de algoritmos, $O(n \\log n)$ a menudo se considera un \"estándar dorado\" para la eficiencia. Algunos problemas no pueden resolverse más rápidamente que en $O(n \\log n)$ bajo el modelo de computación basado en comparaciones, por lo que alcanzar esta complejidad es un indicador de una solución altamente optimizada.\n",
    "\n",
    "**Mergesort**\n",
    "\n",
    "Es un ejemplo clásico de un algoritmo con complejidad $O(n \\log n)$. Funciona de la siguiente manera:\n",
    "1. **Dividir:** Divide el conjunto de datos en dos mitades.\n",
    "2. **Conquistar:** Ordena recursivamente cada mitad.\n",
    "3. **Combinar:** Mezcla las dos mitades ordenadas para formar una única secuencia ordenada.\n",
    "\n",
    "Cada división reduce el tamaño del problema a la mitad, lo que lleva a un componente logarítmico $(\\log n)$ en la complejidad, mientras que la necesidad de ordenar $n$ elementos introduce el componente lineal, resultando en una complejidad total de $O(n \\log n)$.\n",
    "\n",
    "En resumen, la complejidad $O(n \\log n)$ es significativa para el análisis de algoritmos, ya que señala soluciones que son eficientes y escalables, especialmente valiosas en el procesamiento de grandes conjuntos de datos donde las operaciones deben ser óptimas tanto en tiempo como en recursos utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48545479",
   "metadata": {},
   "source": [
    "```\n",
    "Función MergeSort(Arreglo):\n",
    "    Si longitud(Arreglo) > 1:\n",
    "        medio = longitud(Arreglo) / 2\n",
    "        Izquierda = Arreglo[0:medio]\n",
    "        Derecha = Arreglo[medio:longitud(Arreglo)]\n",
    "\n",
    "        MergeSort(Izquierda)\n",
    "        MergeSort(Derecha)\n",
    "\n",
    "        i = j = k = 0\n",
    "\n",
    "        # Mezclar los subarreglos Izquierda y Derecha en Arreglo\n",
    "        Mientras i < longitud(Izquierda) y j < longitud(Derecha):\n",
    "            Si Izquierda[i] < Derecha[j]:\n",
    "                Arreglo[k] = Izquierda[i]\n",
    "                i += 1\n",
    "            Sino:\n",
    "                Arreglo[k] = Derecha[j]\n",
    "                j += 1\n",
    "            k += 1\n",
    "\n",
    "        # Copiar los elementos restantes de Izquierda, si hay alguno\n",
    "        Mientras i < longitud(Izquierda):\n",
    "            Arreglo[k] = Izquierda[i]\n",
    "            i += 1\n",
    "            k += 1\n",
    "\n",
    "        # Copiar los elementos restantes de Derecha, si hay alguno\n",
    "        Mientras j < longitud(Derecha):\n",
    "            Arreglo[k] = Derecha[j]\n",
    "            j += 1\n",
    "            k += 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d63f7e",
   "metadata": {},
   "source": [
    "#### Lista de complejidades temporales\n",
    "\n",
    "La siguiente lista contiene complejidades temporales comunes de los algoritmos:\n",
    "\n",
    "\n",
    "\n",
    "- $O(1)$: El tiempo de ejecución de un algoritmo de tiempo constante no depende del tamaño de la entrada. Un algoritmo típico de tiempo constante es una fórmula directa que calcula la respuesta.\n",
    "\n",
    "- $O(\\log n)$:  Un algoritmo logarítmico suele reducir a la mitad el tamaño de entrada en cada paso. El tiempo de ejecución de dicho algoritmo es logarítmico, porque $\\log_2 n$ es igual al número de veces que `n` debe dividirse por `2` para obtener `1`. Ten en cuenta que la base del logaritmo no se muestra en la complejidad del tiempo.\n",
    "\n",
    "- $O(\\sqrt{n})$ : Un algoritmo de raíz cuadrada es más lento pero más rápido que $O(\\log n)$. Una propiedad especial de las raíces cuadradas es que $\\sqrt{n} = n/\\sqrt{n}$ tal que `n` elementos se pueden dividir en $O(\\sqrt{n})$ bloques de $O(\\sqrt{n})$ elementos.\n",
    "\n",
    "- $O(n)$ Un algoritmo lineal pasa por la entrada un número constante de veces. Esta suele ser la mejor complejidad de tiempo posible, porque generalmente es necesario acceder a cada elemento de entrada al menos una vez antes de informar la respuesta.\n",
    "\n",
    "- $O(n\\log n)$ Esta complejidad temporal a menudo indica que el algoritmo ordena la entrada, porque la complejidad temporal de los algoritmos de clasificación eficientes es $O(n\\log n)$ . Otra posibilidad es que el algoritmo utilice una estructura de datos donde cada operación lleve $O(\\log n)$ veces.\n",
    "\n",
    "- $O(n^2)$ Un algoritmo cuadrático suele contener dos bucles anidados. Es posible revisar todos los pares de elementos de entrada en $O(n^2)$ veces.\n",
    "\n",
    "- $O(n^3)$ Un algoritmo cúbico suele contener tres bucles anidados. Es posible recorrer todos los tripletes de los elementos de entrada en $O(n^3)$ veces.\n",
    "\n",
    "- $O(2^n)$ Esta complejidad temporal a menudo indica que el algoritmo recorre en iteración todos los subconjuntos de elementos de entrada. Por ejemplo, los subconjuntos de $\\{ 1, 2,3\\}$ son $\\emptyset, \\{1\\}, \\{2\\}, \\{3\\}\\{1, 2\\}\\{1, 3\\}, \\{2,3\\}$ y $\\{1, 2, 3\\}$ .\n",
    "\n",
    "- $O(n!)$ Esta complejidad temporal a menudo indica que el algoritmo itera a través de todas las permutaciones de los elementos de entrada. Por ejemplo, las permutaciones de $\\{1,2, 3\\}$ son $(1, 3, 2), (2, 1, 3),(2, 3, 1), (3, 1, 2)$ y $(3 , 2, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc852a3",
   "metadata": {},
   "source": [
    "Un algoritmo es polinómico si su complejidad temporal es como máximo $O(n^k)$ donde `k` es una constante. Todas las complejidades de tiempo anteriores excepto $O(2^n)$ y $O(n!)$ son polinomiales. En la práctica, la constante `k` suele ser pequeña y por lo tanto, una complejidad de tiempo polinómica significa aproximadamente que el algoritmo puede procesar entradas grandes.\n",
    "\n",
    "La mayoría de los algoritmos de varios cursos son polinomiales. \n",
    "\n",
    "Aún así, hay muchos problemas importantes para los cuales no se conoce ningún algoritmo polinomial, es decir, nadie sabe cómo resolverlos de manera eficiente. Los **problemas NP-hard** son un conjunto importante de problemas para los cuales no se conoce ningún algoritmo polinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f12bcb",
   "metadata": {},
   "source": [
    "### Complejidad espacial\n",
    "\n",
    "La complejidad espacial de un algoritmo de ordenación se refiere a la cantidad de memoria adicional que necesita el algoritmo para ejecutar su tarea de ordenar los elementos de una lista o arreglo. Este aspecto es tan importante como la complejidad temporal (el tiempo que tarda en ejecutarse), especialmente en contextos donde los recursos de memoria son limitados o cuando se trabaja con grandes volúmenes de datos. \n",
    "\n",
    "**Ordenamiento por Burbuja, Inserción y Selección**\n",
    "Complejidad espacial: $O(1)$\n",
    "\n",
    "Estos algoritmos de ordenación realizan el ordenamiento \"in situ\", es decir, dentro del propio arreglo sin necesidad de estructuras adicionales significativas, más allá de unas pocas variables temporales. Esto los hace muy eficientes en términos de uso de memoria.\n",
    "\n",
    "**Merge Sort**\n",
    "Complejidad espacial: $O(n)$\n",
    "\n",
    "Aunque Merge Sort es muy eficiente en tiempo, su principal desventaja es su complejidad espacial, ya que necesita un espacio adicional proporcional al tamaño del arreglo de entrada para realizar la mezcla de los subarreglos. Esto se debe a que crea copias de subsecciones del arreglo original para luego mezclarlas ordenadamente.\n",
    "\n",
    "**Quicksort**\n",
    "Complejidad espacial:\n",
    "\n",
    "Peor Caso: $O(n)$ (dependiendo de la implementación)\n",
    "\n",
    "Caso Promedio: $O(\\log n)$\n",
    "\n",
    "Quicksort es un algoritmo de división y conquista que, en su implementación más común, realiza el ordenamiento \"in situ\". Sin embargo, debido a su naturaleza recursiva, el uso de la pila para las llamadas recursivas puede llevar a una complejidad espacial de $O(\\log n)$ en el caso promedio, debido a la profundidad del árbol de recursión. En el peor de los casos, si cada partición divide el arreglo en un subarreglo de un elemento y otro de $n-1$ elementos, la profundidad de la recursión (y por lo tanto el espacio de la pila) puede aumentar a $O(n)$.\n",
    "\n",
    "**Ordenamiento Radix (Radix Sort) y Ordenamiento por Conteo (Counting Sort)**\n",
    "Complejidad espacial: Variable\n",
    "\n",
    "Estos algoritmos no comparativos tienen complejidades espaciales que dependen de los datos de entrada y los detalles específicos de su implementación. Por ejemplo, Counting Sort necesita un espacio adicional para el arreglo de conteo, cuyo tamaño depende del rango de los datos de entrada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462313f",
   "metadata": {},
   "source": [
    "#### Estimación de la eficiencia\n",
    "\n",
    "Calculando la complejidad temporal de un algoritmo, es posible comprobar, antes de implementarlo, que es lo suficientemente eficiente para resolver un problema. El punto de partida de las estimaciones es el hecho de que un ordenador moderno puede realizar unos cientos de millones de operaciones sencillas en un segundo.\n",
    "\n",
    "Por ejemplo, supongamos que el límite de tiempo para un problema es un segundo y el tamaño de entrada es $n =10^{5}$. Si la complejidad del tiempo es $O(n^2)$ el algoritmo realizará aproximadamente  $(10^{5})^2 = 10^{10}$ operaciones. Esto debería tardar al menos unas decenas de segundos, por lo que el algoritmo parece demasiado lento para resolver el problema. \n",
    "\n",
    "Sin embargo, si la complejidad del tiempo es $O(n\\log n)$, solo se tratará de $10^{5}\\log 10^{5} \\approx 1.6 \\cdot 10^{6}$ operaciones y el algoritmo seguramente se ajustará al límite de tiempo.\n",
    "\n",
    "Por otro lado, dado el tamaño de la entrada, podemos intentar adivinar la complejidad temporal requerida del algoritmo que resuelve el problema. \n",
    "\n",
    "**Ejemplo:** [Knowing the complexity in competitive programming](https://www.geeksforgeeks.org/knowing-the-complexity-in-competitive-programming/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ca7a4",
   "metadata": {},
   "source": [
    "Por ejemplo, si el tamaño de entrada es $n = 10^5$ , probablemente se espera que la complejidad temporal del algoritmo sea $O(n)$ o $O(n\\log n)$. Esta información facilita el diseño del algoritmo, porque descarta enfoques que producirían un algoritmo con una peor complejidad temporal.\n",
    "\n",
    "Aun así, es importante recordar que la complejidad del tiempo es sólo una estimación de la eficiencia, porque oculta los factores constantes. Por ejemplo, un algoritmo que se ejecuta en tiempo $O(n)$ puede realizar $n/2$ o $5n$ operaciones, lo que tiene un efecto importante en el tiempo de ejecución real del algoritmo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679fe14",
   "metadata": {},
   "source": [
    "#### Definiciones formales\n",
    "\n",
    "¿Qué significa exactamente que un algoritmo funciona en tiempo $O(f(n)$)? \n",
    "\n",
    "Significa que hay constantes $c$ y $n_0$ tales que el algoritmo realiza como máximo $cf(n)$ operaciones para todas las entradas donde $n \\ge n_0$. Por tanto, la notación $O$ proporciona un límite superior para el tiempo de ejecución del algoritmo para entradas suficientemente grandes.\n",
    "\n",
    "Por ejemplo, es técnicamente correcto decir que la complejidad temporal del siguiente algoritmo es $O(n^2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (int i = 1; i <= n; i++) {\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b3860",
   "metadata": {},
   "source": [
    "Sin embargo, un límite mejor es $O(n)$ y sería muy engañoso dar el límite $O(n^2)$, porque en realidad todo el mundo supone que la notación $O$ se utiliza para dar una estimación precisa de la complejidad del tiempo.\n",
    "\n",
    "También hay otras dos notaciones comunes. La notación $\\Omega$ proporciona un límite inferior para el tiempo de ejecución de un algoritmo. La complejidad temporal de un algoritmo es $\\Omega(f(n))$ , si hay constantes $c$ y $n_0$ tales que el algoritmo realiza al menos $cf(n)$ operaciones para todas las entradas donde $n \\geq n_0$. \n",
    "\n",
    "Finalmente, la notación $\\Theta$ da un límite exacto: la complejidad temporal de un algoritmo es $\\Theta(f(n))$ si es a la vez $O(f(n))$ y $\\Omega(f(n)$. Por ejemplo, dado que la complejidad temporal del algoritmo anterior es $O(n)$ y $\\Omega(n)$ es también $\\Theta(n)$.\n",
    "\n",
    "Podemos utilizar las notaciones anteriores en muchas situaciones, no sólo para referirnos a las complejidades temporales de los algoritmos. Por ejemplo, podríamos decir que un arreglo contiene $O(n)$ valores o que un algoritmo consta de $O(\\log n)$ pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2c501",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "1. Suma máxima de subarreglo\n",
    "\n",
    "      Dada un arreglo de n números, calcula la suma máxima de un subarreglo, es decir, la suma más grande posible de \n",
    "      una secuencia de valores consecutivos en el arreglo. El problema es interesante cuando puede haber valores \n",
    "      negativos en el arreglo.\n",
    "\n",
    "      Una forma sencilla de resolver el problema es revisar todos los subarreglos posibles, calcular la suma de los \n",
    "      valores en cada subarreglo y mantener la suma máxima. El siguiente código implementa este algoritmo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "int mejor = 0;\n",
    "for (int a = 0; a < n; a++) {\n",
    "    for (int b = a; b < n; b++) {\n",
    "        int suma = 0;\n",
    "        for (int k = a; k <= b; k++) {\n",
    "            suma += arreglo[k];\n",
    "        }\n",
    "        mejor = max(mejor,suma);\n",
    "    }\n",
    "}\n",
    "cout << mejor << \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209a2e6",
   "metadata": {},
   "source": [
    "¿Cuál es la complejidad del tiempo del anterior algoritmo?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6672dfa",
   "metadata": {},
   "source": [
    "Es fácil hacer que el algoritmo sea más eficiente eliminando un bucle. Esto es posible calculando la suma al mismo tiempo que se mueve el extremo derecho del subarreglo. El resultado es el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95983b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "int mejor = 0;\n",
    "for (int a = 0; a < n; a++) {\n",
    "    int suma = 0;\n",
    "    for (int k = a; k <= b; k++) {\n",
    "        suma += arreglo[k];\n",
    "        mejor = max(mejor,suma);\n",
    "    }\n",
    "}\n",
    "cout << mejor << \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f1d3c",
   "metadata": {},
   "source": [
    "¿Cuál es la complejidad del tiempo del anterior algoritmo?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51596a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f5cb4",
   "metadata": {},
   "source": [
    "Resulta que es posible resolver el problema en tiempo $O(n)$, lo que significa que solo un bucle es suficiente. La idea es calcular, para cada posición del arreglo, la suma máxima de un subarreglo que termina en esa posición. Después de esto, la respuesta al problema es el máximo de esas sumas.\n",
    "\n",
    "Considera el subproblema de encontrar la suma máxima del subarreglo que termina en la posición $k$. \n",
    "\n",
    "Hay dos posibilidades:\n",
    "\n",
    "1. El subarreglo solo contiene el elemento en la posición $k$.\n",
    "2. El subarreglo consta de un subarreglo que termina en la posición $k -1$, seguido por el elemento en la posición $k$.\n",
    "\n",
    "En el último caso, como queremos encontrar un subarreglo con suma máxima, el subarreglo que termina en la posición $k-1$ también debe tener la suma máxima. Por lo tanto, podemos resolver el problema de manera eficiente calculando la suma máxima del subarreglo para cada posición final de izquierda a derecha.\n",
    "\n",
    "El siguiente código implementa el algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "int mejor = 0, suma = 0;\n",
    "for (int k = 0; k < n; k++) {\n",
    "    suma = max(arreglo[k],suma+arreglo[k]);\n",
    "    mejor = max(mejor,suma);\n",
    "    }\n",
    "    cout << mejor << \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0a958",
   "metadata": {},
   "source": [
    "¿Cuál es la complejidad del tiempo del anterior algoritmo?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693402e",
   "metadata": {},
   "source": [
    "Revisa: [Efficient Algorithms and Intractable Problems ](https://cs170.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2bf30",
   "metadata": {},
   "source": [
    "### Memoización\n",
    "\n",
    "**Ejemplo**: Serie de Fibonacci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int fibonacci(int n, int cache[]) {\n",
    "    if (cache[n] != -1) {\n",
    "        return cache[n];\n",
    "    }\n",
    "\n",
    "    int resultado;\n",
    "    if (n == 0) {\n",
    "        resultado = 0;\n",
    "    } else if (n == 1) {\n",
    "        resultado = 1;\n",
    "    } else {\n",
    "        resultado = fibonacci(n - 1, cache) + fibonacci(n - 2, cache);\n",
    "    }\n",
    "\n",
    "    cache[n] = resultado;\n",
    "    return resultado;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180fb70",
   "metadata": {},
   "source": [
    "#### Complejidad temporal \n",
    "\n",
    "Se verifica el caché para ver si ya hay una respuesta almacenada en el enésimo lugar y se devuelve si está allí. De lo contrario, la suma de `fibonacci (n - 1)` y `fibonacci (n - 2)` se llama de forma recursiva y la suma se establece en una variable. \n",
    "\n",
    "Esta suma se coloca en el arreglo de caché en el lugar $n$ y luego se devuelve el valor de la suma. Con esta solución en memoización, cada vez que se llama a `fibonacci(n)` y $n$ ya se ha resuelto una vez, `cache[n]` ya contendrá la respuesta y la devolverá. \n",
    "\n",
    "La complejidad temporal de llamar recursivamente a `fibonacci (n - 1) + fibonacci (n - 2)` es $O(2^n)$ y mejora mucho con la memoización: $O(n)$.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32b923",
   "metadata": {},
   "source": [
    "#### Complejidad espacial  \n",
    "\n",
    "Utilizando la técnica de memorización, cada valor de 'fibonacci' se calculará solo una vez. Entonces, la complejidad del espacio será $O(n)$, donde 'n' es el número de entrada para 'fibonacci' (el arreglo para la memorización contendrá 'n' números).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0916c1",
   "metadata": {},
   "source": [
    "### Tabulación\n",
    "\n",
    "La tabulación es similar en el sentido de que crea un caché, pero el enfoque es diferente. Un algoritmo de tabulación se centra en llenar las entradas de la caché, hasta alcanzar el valor objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int fibonacci(int n) {\n",
    "    if (n == 0) {\n",
    "        return 0;\n",
    "    } else if (n == 1) {\n",
    "        return 1;\n",
    "    } else {\n",
    "        int tabla[n + 1];\n",
    "        tabla[0] = 0;\n",
    "        tabla[1] = 1;\n",
    "        for (int i = 2; i <= n; i++) {\n",
    "            tabla[i] = tabla[i - 1] + tabla[i - 2];\n",
    "        }\n",
    "        return tabla[n];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8469e",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "\n",
    "* Analiza la complejidad temporal y espacial de la tabulación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95256976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
